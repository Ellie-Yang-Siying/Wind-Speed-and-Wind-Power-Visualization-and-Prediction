{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 20:08:10.889470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 20:08:11.233560: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-12 20:08:11.233608: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-12 20:08:12.687522: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-12 20:08:12.687855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-12 20:08:12.687885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 30, 40, 50])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据集特征\n",
    "n_back = 200\n",
    "n_out = 16\n",
    "n_pre = n_out*15*2\n",
    "n_feature = 2\n",
    "\n",
    "train_day = 6\n",
    "validation_day = 4\n",
    "test_day = 3\n",
    "\n",
    "# 神经网络参数\n",
    "import numpy as np\n",
    "units = np.arange(20, 60, 10)\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       speed    power\n",
      "date                                 \n",
      "2015-10-01 00:00:00  0.39627  0.38065\n",
      "2015-10-01 00:00:30  0.39592  0.36943\n",
      "2015-10-01 00:01:00  0.39538  0.38529\n",
      "2015-10-01 00:01:30  0.39579  0.38892\n",
      "2015-10-01 00:02:00  0.39627  0.41220\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../wind_preprocessed.csv', header=0, index_col=0).query('day<14')\n",
    "data = dataset[['speed_moveavg', 'power_moveavg']].rename(columns={'power_moveavg':'power', 'speed_moveavg':'speed'})\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data.values\n",
    "values = values.astype('float32')\n",
    "\n",
    "def series_to_supervised(data, n_in, n_out, colname, dropnan=True):\n",
    "    n_vars = colname\n",
    "    # n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('%s(t-%d)' % (j, i)) for j in n_vars]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out, 15*2):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('%s(t)' % (j)) for j in n_vars]\n",
    "        else:\n",
    "            names += [('%s(t+%d)' % (j, i)) for j in n_vars]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# 构建成监督学习问题\n",
    "reframed = series_to_supervised(values, n_back, n_pre, ['speed', 'power'])\n",
    "# 丢弃我们不想预测的列\n",
    "for i in range(0, n_pre, 15*2):\n",
    "    if i == 0:\n",
    "        colname = 'speed(t)'\n",
    "    else:\n",
    "        colname = f'speed(t+{i})'\n",
    "    reframed.drop(colname, axis=1, inplace=True)\n",
    "# print(reframed.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17280, 200, 2) (17280, 16)\n",
      "(11520, 200, 2) (11520, 16)\n",
      "(8640, 200, 2) (8640, 16)\n"
     ]
    }
   ],
   "source": [
    "# 分割为训练集和测试集\n",
    "values = reframed.values\n",
    "n_train = train_day*24*60*2\n",
    "n_validation = validation_day*24*60*2\n",
    "n_test = test_day*24*60*2\n",
    "train = values[:n_train, :]\n",
    "validation = values[n_train:n_train+n_validation, :]\n",
    "test = values[-n_test:, :]\n",
    "# 分为输入输出\n",
    "n_obs = n_back * n_feature\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_out:]\n",
    "validation_X, validation_y = validation[:, :n_obs], validation[:, -n_out:]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_out:]\n",
    "# 重塑成3D格式 [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_back, n_feature))\n",
    "validation_X = validation_X.reshape((validation_X.shape[0], n_back, n_feature))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_back, n_feature))\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(validation_X.shape, validation_y.shape)\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 20:08:16.570694: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-12 20:08:16.570944: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-12 20:08:16.571024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-VC833VJB): /proc/driver/nvidia/version does not exist\n",
      "2023-01-12 20:08:16.572131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 191, 100)          2100      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 182, 100)          100100    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 60, 100)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 51, 160)           160160    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 42, 160)           256160    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 160)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                2576      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 521,096\n",
      "Trainable params: 521,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model_m = Sequential()\n",
    "# # model_m.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(input_shape,)))\n",
    "# model_m.add(Conv1D(100, 10, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "# model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "# model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "# model_m.add(GlobalAveragePooling1D())\n",
    "# model_m.add(Dropout(0.5))\n",
    "# model_m.add(Dense(n_out, activation='softmax'))\n",
    "# print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_width = 200\n",
    "n_features = 1\n",
    "epoch_num = 500\n",
    "verbose_set = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_10 (Conv1D)          (None, 199, 64)           192       \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 199, 64)           0         \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 197, 64)           12352     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 98, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                313650    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,245\n",
      "Trainable params: 326,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " None\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200, 1), dtype=tf.float32, name='conv1d_10_input'), name='conv1d_10_input', description=\"created by layer 'conv1d_10_input'\"), but it was called on an input with incompatible shape (None, 200, 2).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n    \n    Input 0 of layer \"conv1d_10\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 200, 2)\n    \n    Call arguments received by layer 'sequential_4' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 200, 2), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,model\u001b[39m.\u001b[39msummary())\n\u001b[1;32m     46\u001b[0m \u001b[39m# X为输入数据，y为数据标签；batch_size：每次梯度更新的样本数，默认为32。\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# verbose: 0,1,2. 0=训练过程无输出，1=显示训练过程进度条，2=每训练一个epoch打印一次信息\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_X, train_y, batch_size\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m, epochs\u001b[39m=\u001b[39;49mepoch_num, verbose\u001b[39m=\u001b[39;49mverbose_set,validation_data\u001b[39m=\u001b[39;49m(validation_X, validation_y))\n\u001b[1;32m     50\u001b[0m \u001b[39m# 保存模型及训练历史\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileej9z46ip.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/ellahu/anaconda3/envs/bigdata/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n    \n    Input 0 of layer \"conv1d_10\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 200, 2)\n    \n    Call arguments received by layer 'sequential_4' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 200, 2), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import  Dense,Dropout,Flatten\n",
    "model = Sequential()\n",
    "    \n",
    "# 对于一维卷积来说，data_format='channels_last'是默认配置，该API的规则如下：\n",
    "# 输入形状为：(batch, steps, channels)；输出形状为：(batch, new_steps, filters)，padding和strides的变化会导致new_steps变化\n",
    "# 如果设置为data_format = 'channels_first'，则要求输入形状为： (batch, channels, steps).\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu',\n",
    "                     strides=1, padding='valid', data_format='channels_last',\n",
    "                     input_shape=(sw_width, n_features)))\n",
    "model.add(Dropout((0.5)))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
    "                     strides=1, padding='valid', data_format='channels_last',\n",
    "                     input_shape=(sw_width, n_features)))\n",
    "\n",
    "# model.add(Dropout((0.5)))\n",
    "\n",
    "# model.add(Dense(units=50, activation='relu',\n",
    "#                 use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',))\n",
    "\n",
    "# 对于一维池化层来说，data_format='channels_last'是默认配置，该API的规则如下：\n",
    "# 3D 张量的输入形状为: (batch_size, steps, features)；输出3D张量的形状为：(batch_size, downsampled_steps, features)\n",
    "# 如果设置为data_format = 'channels_first'，则要求输入形状为：(batch_size, features, steps)\n",
    "model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid', \n",
    "                           data_format='channels_last')) \n",
    "    \n",
    "# data_format参数的作用是在将模型从一种数据格式切换到另一种数据格式时保留权重顺序。默认为channels_last。\n",
    "# 如果设置为channels_last，那么数据输入形状应为：（batch，…，channels）；如果设置为channels_first，那么数据输入形状应该为（batch，channels，…）\n",
    "# 输出为（batch, 之后参数尺寸的乘积）\n",
    "     \n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense执行以下操作：output=activation（dot（input，kernel）+bias），\n",
    "# 其中,activation是激活函数，kernel是由层创建的权重矩阵，bias是由层创建的偏移向量（仅当use_bias为True时适用）。\n",
    "# 2D 输入：(batch_size, input_dim)；对应 2D 输出：(batch_size, units)\n",
    "model.add(Dense(units=50, activation='relu',\n",
    "                use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',))\n",
    "model.add(Dropout((0.5)))\n",
    "# 因为要预测下一个时间步的值，因此units设置为1\n",
    "model.add(Dense(units=1))\n",
    "    \n",
    "# 配置模型\n",
    "model.compile(optimizer='adam', loss='mse',\n",
    "                  loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "print('\\n',model.summary())\n",
    "# X为输入数据，y为数据标签；batch_size：每次梯度更新的样本数，默认为32。\n",
    "# verbose: 0,1,2. 0=训练过程无输出，1=显示训练过程进度条，2=每训练一个epoch打印一次信息\n",
    "    \n",
    "history = model.fit(train_X, train_y, batch_size=400, epochs=epoch_num, verbose=verbose_set,validation_data=(validation_X, validation_y))\n",
    "# 保存模型及训练历史\n",
    "import pickle\n",
    "model.save_weights(f'CNN_1.h5')\n",
    "with open(f'CNN_1_hist.pickle', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5770e3b25a98de75bce43f2f8d39d9898bdd15f26b6dfc4856e2120567bac70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
